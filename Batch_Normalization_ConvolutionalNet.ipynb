{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Practicing Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### By building a 20 layers convolutional neural network with a fully connected layer to classify handwritten digits.\n",
    "\n",
    "NOT GOOD FOR CLASSIFYING MNIST DIGITS BUT GOOD FOR PRACTICE \n",
    "since Batch Normalization works good with deeper neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Loading Tensorflow and downloading Mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to create fully connected layers without batch normalization\n",
    "\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to create convolutional layers (without pooling) without Batch Normalization\n",
    "\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    strides = 2 if layer_depth % 3 ==0 else 1\n",
    "    layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides=strides, padding=\"same\", activation= tf.nn.relu)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69067, Validation accuracy: 0.09900\n",
      "Batch: 25: Training loss: 0.36572, Training accuracy: 0.04688\n",
      "Batch: 50: Training loss: 0.32663, Training accuracy: 0.14062\n",
      "Batch: 75: Training loss: 0.32599, Training accuracy: 0.07812\n",
      "Batch: 100: Validation loss: 0.32638, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32680, Training accuracy: 0.07812\n",
      "Batch: 150: Training loss: 0.32655, Training accuracy: 0.06250\n",
      "Batch: 175: Training loss: 0.32512, Training accuracy: 0.12500\n",
      "Batch: 200: Validation loss: 0.32640, Validation accuracy: 0.09900\n",
      "Batch: 225: Training loss: 0.32114, Training accuracy: 0.20312\n",
      "Batch: 250: Training loss: 0.32470, Training accuracy: 0.12500\n",
      "Batch: 275: Training loss: 0.32804, Training accuracy: 0.06250\n",
      "Batch: 300: Validation loss: 0.32597, Validation accuracy: 0.09900\n",
      "Batch: 325: Training loss: 0.32647, Training accuracy: 0.06250\n",
      "Batch: 350: Training loss: 0.32461, Training accuracy: 0.15625\n",
      "Batch: 375: Training loss: 0.32900, Training accuracy: 0.06250\n",
      "Batch: 400: Validation loss: 0.32559, Validation accuracy: 0.09760\n",
      "Batch: 425: Training loss: 0.32604, Training accuracy: 0.03125\n",
      "Batch: 450: Training loss: 0.32605, Training accuracy: 0.07812\n",
      "Batch: 475: Training loss: 0.32633, Training accuracy: 0.07812\n",
      "Batch: 500: Validation loss: 0.32533, Validation accuracy: 0.11260\n",
      "Batch: 525: Training loss: 0.32370, Training accuracy: 0.10938\n",
      "Batch: 550: Training loss: 0.32568, Training accuracy: 0.14062\n",
      "Batch: 575: Training loss: 0.32590, Training accuracy: 0.10938\n",
      "Batch: 600: Validation loss: 0.32578, Validation accuracy: 0.09860\n",
      "Batch: 625: Training loss: 0.32537, Training accuracy: 0.09375\n",
      "Batch: 650: Training loss: 0.32583, Training accuracy: 0.10938\n",
      "Batch: 675: Training loss: 0.32538, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32562, Validation accuracy: 0.10700\n",
      "Batch: 725: Training loss: 0.32717, Training accuracy: 0.14062\n",
      "Batch: 750: Training loss: 0.32713, Training accuracy: 0.07812\n",
      "Batch: 775: Training loss: 0.32434, Training accuracy: 0.09375\n",
      "Final validation accuracy: 0.11260\n",
      "Final test accuracy: 0.11350\n",
      "Accuracy on 100 samples: 0.14\n"
     ]
    }
   ],
   "source": [
    "### Building and training the network on the Mnist dataset, while displaying loss and accuracy\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    #20 convolutional filters\n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "        \n",
    "    layer_shape= layer.get_shape().as_list()    \n",
    "    layer = tf.reshape(layer, shape=[-1, layer_shape[1]*layer_shape[2]*layer_shape[3]])\n",
    "    \n",
    "    #fully connected with relu activation\n",
    "    fc_layer = fully_connected(layer, 100)\n",
    "    \n",
    "    #logits - fully connected without relu activation\n",
    "    logits = tf.layers.dense(fc_layer, 10)\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(opt, {inputs: batch_xs, \n",
    "                                 labels: batch_ys})\n",
    "            \n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)    \n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Batch Normalization Accuracy is lower then 15%;\n",
    "Now i'm going to train the same network using batch Normalization to over 90% in the same number of Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH NORMALIZATION: Higher Level Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fully connected layer with batch normalization\n",
    "\n",
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    #not using bias since batch normalization implement gamma*layer + beta where beta replace the biases units.\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    #adding training=is_training since the network needs to know how to update the pop. statistics\n",
    "    layer = tf.layers.batch_normalization(layer, epsilon=0.001, beta_initializer=tf.zeros_initializer(), \n",
    "                                          gamma_initializer=tf.ones_initializer(), training=is_training)\n",
    "    \n",
    "    #adding the activation function\n",
    "    out = tf.nn.relu(layer)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convolutional Layer with BN\n",
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \n",
    "    strides = 2 if layer_depth % 3 ==0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides=strides, padding='same', activation= None, use_bias=False)\n",
    "    \n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return conv_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Batch Normalization\n",
    "**1** Placeholder for is_training since it will be set to True during training and set to False during inference\n",
    "**2** is_training is passed in the conv_layer and in the fully_connected layer\n",
    "**3** optimizer inside the tf.control_dependencies since it has to update pop. statistics of the normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69178, Validation accuracy: 0.09860\n",
      "Batch: 25: Training loss: 0.60032, Training accuracy: 0.07812\n",
      "Batch: 50: Training loss: 0.49489, Training accuracy: 0.12500\n",
      "Batch: 75: Training loss: 0.41710, Training accuracy: 0.12500\n",
      "Batch: 100: Validation loss: 0.37155, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.34586, Training accuracy: 0.12500\n",
      "Batch: 150: Training loss: 0.33138, Training accuracy: 0.15625\n",
      "Batch: 175: Training loss: 0.35367, Training accuracy: 0.12500\n",
      "Batch: 200: Validation loss: 0.35802, Validation accuracy: 0.14740\n",
      "Batch: 225: Training loss: 0.25998, Training accuracy: 0.46875\n",
      "Batch: 250: Training loss: 0.23626, Training accuracy: 0.51562\n",
      "Batch: 275: Training loss: 0.18696, Training accuracy: 0.60938\n",
      "Batch: 300: Validation loss: 0.15098, Validation accuracy: 0.75120\n",
      "Batch: 325: Training loss: 0.07846, Training accuracy: 0.87500\n",
      "Batch: 350: Training loss: 0.09178, Training accuracy: 0.82812\n",
      "Batch: 375: Training loss: 0.09913, Training accuracy: 0.85938\n",
      "Batch: 400: Validation loss: 0.06519, Validation accuracy: 0.90240\n",
      "Batch: 425: Training loss: 0.06764, Training accuracy: 0.87500\n",
      "Batch: 450: Training loss: 0.10690, Training accuracy: 0.90625\n",
      "Batch: 475: Training loss: 0.01458, Training accuracy: 0.98438\n",
      "Batch: 500: Validation loss: 0.03870, Validation accuracy: 0.94860\n",
      "Batch: 525: Training loss: 0.04785, Training accuracy: 0.92188\n",
      "Batch: 550: Training loss: 0.06010, Training accuracy: 0.92188\n",
      "Batch: 575: Training loss: 0.04038, Training accuracy: 0.95312\n",
      "Batch: 600: Validation loss: 0.08740, Validation accuracy: 0.89280\n",
      "Batch: 625: Training loss: 0.05582, Training accuracy: 0.92188\n",
      "Batch: 650: Training loss: 0.02143, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.01115, Training accuracy: 0.98438\n",
      "Batch: 700: Validation loss: 0.06566, Validation accuracy: 0.91520\n",
      "Batch: 725: Training loss: 0.08521, Training accuracy: 0.89062\n",
      "Batch: 750: Training loss: 0.01286, Training accuracy: 0.98438\n",
      "Batch: 775: Training loss: 0.03955, Training accuracy: 0.93750\n",
      "Final validation accuracy: 0.95140\n",
      "Final test accuracy: 0.95090\n",
      "Accuracy on 100 samples: 0.99\n"
     ]
    }
   ],
   "source": [
    "#When training, the moving_mean and moving_variance need to be updated. \n",
    "#By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, \n",
    "#so they need to be added as a dependency to the train_op\n",
    "\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #20 convolutional filters\n",
    "    layer = inputs\n",
    "    \n",
    "\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "        \n",
    "    layer_shape= layer.get_shape().as_list()    \n",
    "    layer = tf.reshape(layer, shape=[-1, layer_shape[1]*layer_shape[2]*\n",
    "                                     layer_shape[3]])\n",
    "    \n",
    "    #fully connected with batch normalization\n",
    "    fc_layer = fully_connected(layer, 100, is_training)\n",
    "    \n",
    "    #logits - fully connected without relu activation\n",
    "    logits = tf.layers.dense(fc_layer, 10)\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels))\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(opt, {inputs: batch_xs, \n",
    "                                    labels: batch_ys, is_training:True})\n",
    "            \n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], \n",
    "                                     {inputs: mnist.validation.images, \n",
    "                                      labels: mnist.validation.labels, is_training:False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(\n",
    "                    batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], \n",
    "                                     {inputs: batch_xs, labels: batch_ys, is_training:False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(\n",
    "                    batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, is_training:False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels, is_training:False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: \n",
    "                                                    [mnist.test.images[i]],\n",
    "                                                    labels: \n",
    "                                                    [mnist.test.labels[i]], \n",
    "                                                    is_training:False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)    \n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)\n",
    "    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Batch Normalization i got an excellent performance without changes in the network and just one epoch of 800 batches! Accuracy is 99%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH NORMALIZATION: Lower Level Implementation\n",
    "\n",
    "tf.nn.batch_normalization : normalizes a tensor by mean and variance, and applies (optionally) a scale γ to it, as well as an offset β."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create fully_connected layer. \n",
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \n",
    "    #fc_layer with no bias and no activation in order to implement batch_normalization \n",
    "    fc_layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    #beta and gamma are the parameters of the equation y= gamma*x + beta\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "      \n",
    "   \n",
    "    #mean and variance of the population \n",
    "    pop_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    \n",
    "    def batch_norm_training():\n",
    "        \n",
    "        decay= 0.99\n",
    "        #extracting mean and variance from the current batch using tf.moments\n",
    "        batch_mean, batch_variance = tf.nn.moments(fc_layer, [0])\n",
    "        \n",
    "        #updating mean and variance for the inference phase\n",
    "        train_mean = tf.assign(pop_mean, (pop_mean * decay) + batch_mean*(1-decay))\n",
    "        train_variance = tf.assign(pop_variance, (pop_variance * decay) + batch_variance*(1-decay))     \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(fc_layer, mean=batch_mean, variance=batch_variance, \n",
    "                                             offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "    \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(fc_layer, mean=pop_mean, variance=pop_variance, offset=beta, \n",
    "                                         scale=gamma, variance_epsilon=epsilon)\n",
    "    \n",
    "    batch_norm_out = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    \n",
    "    #returning the normalized value after the relu activation \n",
    "    return tf.nn.relu(batch_norm_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create convolutional_layers with batch normalization.\n",
    "#using layer depth to create the filter of the network and the strides is not good for the CNN but it's ok \n",
    "#for the purpose of practicing with BN\n",
    "\n",
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \n",
    "    #as before\n",
    "    strides = 2 if layer_depth %3 ==0 else 1\n",
    "    \n",
    "    #defining the filter/feature map\n",
    "    in_ch = prev_layer.get_shape().as_list()[3]\n",
    "    out_ch = layer_depth*4\n",
    "    weights = tf.Variable(tf.truncated_normal([3,3, in_ch, out_ch], stddev=0.05))\n",
    "    \n",
    "    #conv layer\n",
    "    conv_layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides,strides,1], padding='SAME')\n",
    "    \n",
    "    #as before\n",
    "    gamma = tf.Variable(tf.ones([out_ch]))\n",
    "    beta = tf.Variable(tf.zeros([out_ch]))\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([out_ch]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([out_ch]), trainable=False)\n",
    "    \n",
    "    epsilon= 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        #as before\n",
    "                \n",
    "        decay= 0.99\n",
    "        #extracting mean and variance from the current batch using tf.moments\n",
    "        batch_mean, batch_variance = tf.nn.moments(conv_layer, [0, 1, 2], keep_dims=False)\n",
    "        \n",
    "        #updating mean and variance for the inference phase\n",
    "        train_mean = tf.assign(pop_mean, (pop_mean * decay) + batch_mean*(1-decay))\n",
    "        train_variance = tf.assign(pop_variance, (pop_variance * decay) + batch_variance*(1-decay))     \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(conv_layer, mean=batch_mean, variance=batch_variance, \n",
    "                                             offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "        \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(conv_layer, mean=pop_mean, variance=pop_variance, offset=beta, \n",
    "                                         scale=gamma, variance_epsilon=epsilon)\n",
    "    \n",
    "    batch_norm_out = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    \n",
    "    #returning the normalized value after the relu activation \n",
    "    return tf.nn.relu(batch_norm_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69078, Validation accuracy: 0.10700\n",
      "Batch: 25: Training loss: 0.58244, Training accuracy: 0.10938\n",
      "Batch: 50: Training loss: 0.47270, Training accuracy: 0.04688\n",
      "Batch: 75: Training loss: 0.39788, Training accuracy: 0.07812\n",
      "Batch: 100: Validation loss: 0.36073, Validation accuracy: 0.10700\n",
      "Batch: 125: Training loss: 0.35411, Training accuracy: 0.03125\n",
      "Batch: 150: Training loss: 0.32505, Training accuracy: 0.14062\n",
      "Batch: 175: Training loss: 0.35393, Training accuracy: 0.10938\n",
      "Batch: 200: Validation loss: 0.40405, Validation accuracy: 0.11260\n",
      "Batch: 225: Training loss: 0.47686, Training accuracy: 0.20312\n",
      "Batch: 250: Training loss: 0.68870, Training accuracy: 0.10938\n",
      "Batch: 275: Training loss: 0.41925, Training accuracy: 0.15625\n",
      "Batch: 300: Validation loss: 0.82895, Validation accuracy: 0.11440\n",
      "Batch: 325: Training loss: 0.76750, Training accuracy: 0.07812\n",
      "Batch: 350: Training loss: 0.46322, Training accuracy: 0.28125\n",
      "Batch: 375: Training loss: 0.39270, Training accuracy: 0.37500\n",
      "Batch: 400: Validation loss: 0.39302, Validation accuracy: 0.36840\n",
      "Batch: 425: Training loss: 0.41512, Training accuracy: 0.40625\n",
      "Batch: 450: Training loss: 0.36016, Training accuracy: 0.54688\n",
      "Batch: 475: Training loss: 0.58151, Training accuracy: 0.46875\n",
      "Batch: 500: Validation loss: 0.18255, Validation accuracy: 0.72000\n",
      "Batch: 525: Training loss: 0.25641, Training accuracy: 0.67188\n",
      "Batch: 550: Training loss: 0.13348, Training accuracy: 0.79688\n",
      "Batch: 575: Training loss: 0.26048, Training accuracy: 0.67188\n",
      "Batch: 600: Validation loss: 0.05418, Validation accuracy: 0.91400\n",
      "Batch: 625: Training loss: 0.03811, Training accuracy: 0.95312\n",
      "Batch: 650: Training loss: 0.03032, Training accuracy: 0.95312\n",
      "Batch: 675: Training loss: 0.02503, Training accuracy: 0.96875\n",
      "Batch: 700: Validation loss: 0.02738, Validation accuracy: 0.96080\n",
      "Batch: 725: Training loss: 0.02848, Training accuracy: 0.96875\n",
      "Batch: 750: Training loss: 0.01631, Training accuracy: 0.93750\n",
      "Batch: 775: Training loss: 0.03970, Training accuracy: 0.93750\n",
      "Final validation accuracy: 0.94620\n",
      "Final test accuracy: 0.95230\n",
      "Accuracy on 100 samples: 0.98\n"
     ]
    }
   ],
   "source": [
    "#removing tf.control_dependencies since it has already been used in the functions above\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #20 convolutional filters\n",
    "    layer = inputs\n",
    "    \n",
    "\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "        \n",
    "    layer_shape= layer.get_shape().as_list()    \n",
    "    layer = tf.reshape(layer, shape=[-1, layer_shape[1]*layer_shape[2]*layer_shape[3]])\n",
    "    \n",
    "    #fully connected with batch normalization\n",
    "    fc_layer = fully_connected(layer, 100, is_training)\n",
    "    \n",
    "    #logits - fully connected without relu activation\n",
    "    logits = tf.layers.dense(fc_layer, 10)\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(opt, {inputs: batch_xs, \n",
    "                                    labels: batch_ys, is_training:True})\n",
    "            \n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels, is_training:False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training:False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        \n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, is_training:False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels, is_training:False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]], is_training:False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)    \n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As before the model trained with Batch Normalization reaches greater performances and higher accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
